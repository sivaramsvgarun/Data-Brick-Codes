{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "eb4c9a24-3db4-4473-aed3-9ef471e30570",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763196342065}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Excel file-a pandas la read panrathu\n",
    "file_path = \"/Volumes/workspace/default/practice-files/sql sample.xlsx\"\n",
    "pandas_df = pd.read_excel(file_path)\n",
    "\n",
    "# Pandas DataFrame-a Spark DataFrame-ku convert panrathu\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Display panrathu\n",
    "# display(df)\n",
    "\n",
    "# Schema-va paakalam\n",
    "df.printSchema()\n",
    "\n",
    "# Row count check panalam\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "display(df.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "75634edd-f22c-4f40-a699-b99148f3fbce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    'Read CSV File into DataFrame') \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.sql.caseSensitive\", \"True\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"s3://siva-databricks-files/cyber-01/*\")\n",
    "\n",
    "df1 = df.withColumn(\n",
    "    \"as_date\",\n",
    "    to_timestamp(col(\"ITEM_DATE\"), \"dd-MMM-yy hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "def getInterval(time):\n",
    "    start = int(time.split(\":\")[0])\n",
    "    return f\"{start}-{start+1}\"\n",
    "\n",
    "getIntervalUdf = udf(getInterval, StringType())\n",
    "\n",
    "df2 = df1.withColumn(\"date\", date_format(col('as_date'), 'yyyy-MM-dd')) \\\n",
    "         .withColumn(\"time\", date_format(col('as_date'), 'HH:mm:ss a')) \\\n",
    "         .withColumn(\"Interval\", getIntervalUdf(\"time\")) \\\n",
    "         .withColumn(\"account_id\", col('ACCOUNT_ID').cast(IntegerType()))\n",
    "\n",
    "item_df1 = df2.groupBy('date','account_id','Interval').agg(count('*').alias('msg_per_hr'))         \n",
    "\n",
    "# display(item_df1)\n",
    "\n",
    "df3 = item_df1.withColumn(\"year\", year(col('date').cast(\"timestamp\")))\\\n",
    "    .withColumn('month', month(col('date').cast('timestamp')))\\\n",
    "    .withColumn(\"day\", dayofmonth(col('date').cast('timestamp')))\\\n",
    "    .withColumn(\"hour\", split(col('Interval'),'-')[0].cast(IntegerType()))\\\n",
    "    .withColumn(\"hour_interval\", col('Interval'))\\\n",
    "    .withColumn(\"dayname\", date_format(df2.date, \"EEEE\"))\\\n",
    "    .withColumn(\"week\",weekofyear(col('date')))\n",
    "display(df3)    \n",
    "\n",
    "\n",
    "# df4=df3.withColumn(\"daytime\",when((df3.hour >6) & (df3.hour <= 12), 'Morning').when((df3.hour > 12) & (df3.hour <= 17), 'Afternoon').when((df3.hour > 17) & (df3.hour <= 20), 'Evening').otherwise('Night'))\n",
    "# display(df4)\n",
    "\n",
    "# s3_bucket = 'siva-databricks-files'\n",
    "# path = \"s3://siva-databricks-files/cyber-01/\"\n",
    "\n",
    "\n",
    "# # df4.write.option(\"compression\", \"snappy\").option(\"overwriteSchema\", \"true\").option(\"path\", f\"3://{s3_bucket}/cyber-01/sgc_items/\").mode(\"overwrite\").format(\"parquet\")\n",
    "# df4.write.parquet(f\"s3://{s3_bucket}/cyber-01/sgc_items/\")\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6bb68191-5b34-42e8-8937-23466b75ef3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    'Read CSV File into DataFrame'\n",
    ").config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    ".config(\"spark.sql.caseSensitive\", \"True\") \\\n",
    ".config(\"spark.sql.source.commitProtocolClass\",\n",
    "        \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "ipath = \"s3://siva-databricks-files/cyber-01/sgc_items/part-00000-tid-4466780840797933117-09085b49-498d-48c8-a2de-b8a028ef6d9d-174-1.c000.snappy.parquet\"\n",
    "df = spark.read.parquet(ipath)\n",
    "# day = df.groupBy('date','Interval', 'hour', 'account_id', 'msg_per_hr').count()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "99218959-ce82-4cfa-a69a-316b93e5d556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a='a3b2c4'\n",
    "output = \".join(map(lamda x:x[0]*int (x[1], zip (s:[::2], s[1::2]))))\"\n",
    "\n",
    "s = 'a3b2c4'\n",
    "output = ''.join(map(lambda x: x[0] * int(x[1]), zip(s[::2], s[1::2])))\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Khafka Project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
